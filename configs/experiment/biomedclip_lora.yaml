# @package _global_

# to execute this experiment run:
# python train.py experiment=test

#######################
# Override defaults   #
#######################

# python src/train.py experiment=biomed_clipseg datamodule=img_txt_mask/camus experiment_name="biomed_clipseg_camus_baseline" prompt_type="p7"

defaults:
  - override /datamodule: ???
  - override /model: biomedclip_lora.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml
  - override /logger: wandb.yaml
  - override /extras: default.yaml

experiment_name: ??? # experiment name for logging and checkpointing

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters


trainer:
  min_epochs: 10
  max_epochs: 100
  gradient_clip_val: 0.5
  accelerator: gpu
  devices: [0]
  precision: 16-mixed


logger:
  wandb:
    project: medvlsm
    tags: ${tags}
    name: ${experiment_name}


############################
# Additional parameters    #
############################

#####
# Default prompts for each models, adapted from BiomedCLIP model: 
# https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224
#####

img_size: [224, 224] # 224 for Biomedclipseg
tokenizer_type: biomedclip # clipseg or biomedclip
context_length: 256 # 77 for clipseg, 256 for biomedclip
tags: [BiomedCLIPSeg+LoRA]
seed: 42

img_mean: [0.48145466, 0.4578275, 0.40821073]
img_std: [0.26862954, 0.26130258, 0.27577711]

img_transforms:
  _target_: torchvision.transforms.Compose
  transforms:
    - _target_: torchvision.transforms.Resize
      size: ${img_size}
    - _target_: torchvision.transforms.ToTensor
    - _target_: torchvision.transforms.Normalize
      mean: ${img_mean}
      std: ${img_std}

mask_transforms:
  _target_: torchvision.transforms.Compose
  transforms:
    - _target_: torchvision.transforms.Resize
      size: ${img_size}
      interpolation: 0 # for nearest exact of pillow constant
    - _target_: torchvision.transforms.ToTensor

# Image pre-processing configs
train_img_transforms: ${img_transforms}
train_mask_transforms: ${mask_transforms}

val_img_transforms: ${img_transforms}
val_mask_transforms: ${mask_transforms}

test_img_transforms: ${img_transforms}
test_mask_transforms: ${mask_transforms}

model:
  net:
    lora:
      r: 8
      lora_alpha: 16
      lora_dropout: 0.1

  optimizer:
    lr: 1e-4 # LoRA often works better with a slightly different LR

datamodule:
  batch_size: 64 # Adjust if you get memory errors